{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>モジュールのインポート</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretreatment import pretreatment\n",
    "from search_tweet_and_save import search_tweets_and_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ライブラリのインポート</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st6322011/anaconda3/envs/NLP/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, LukeConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ツイートデータの取得、ロード,前処理.ここでアカウントの情報を設定する必要があります</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name=os.environ.get('AUTH_INFO_1')\n",
    "mail=os.environ.get('AUTH_INFO_2')\n",
    "password=os.environ.get('TWIKIT_PASSWORD')\n",
    "pickle_name = \"tweets\"\n",
    "search_tweets_and_save(query=\"自然言語処理 lang:ja\", user_name=user_name, mail=mail, password=password, pickle_name=pickle_name, max_tweets=100, save_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pickle_name}.pkl', 'rb') as file:\n",
    "    texts = pickle.load(file)\n",
    "\n",
    "texts_after_treat :List[str]= []\n",
    "for text in texts :\n",
    "    text = pretreatment(text)\n",
    "    texts_after_treat.append(text)\n",
    "    #print(text)\n",
    "    #print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>学習済みモデルとトークナイザーをロード</h1>\n",
    "\n",
    "- https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime : で公開されていて、だれでも使うことができます\n",
    "- URL先の右上の「Use in Transformers」をクリックすると、下記のコードを取得できます\n",
    "- このHugging Faceでは他にも様々な学習済みモデルを利用することができます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st6322011/anaconda3/envs/NLP/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ツイートデータを感情分析する前処理</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['喜び', '悲しみ', '期待', '驚き', '怒り', '恐れ', '嫌悪', '信頼','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>喜び</th>\n",
       "      <th>悲しみ</th>\n",
       "      <th>期待</th>\n",
       "      <th>驚き</th>\n",
       "      <th>怒り</th>\n",
       "      <th>恐れ</th>\n",
       "      <th>嫌悪</th>\n",
       "      <th>信頼</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [喜び, 悲しみ, 期待, 驚き, 怒り, 恐れ, 嫌悪, 信頼, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   喜び      0 non-null      object\n",
      " 1   悲しみ     0 non-null      object\n",
      " 2   期待      0 non-null      object\n",
      " 3   驚き      0 non-null      object\n",
      " 4   怒り      0 non-null      object\n",
      " 5   恐れ      0 non-null      object\n",
      " 6   嫌悪      0 non-null      object\n",
      " 7   信頼      0 non-null      object\n",
      " 8   text    0 non-null      object\n",
      "dtypes: object(9)\n",
      "memory usage: 132.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#この時点で、八個の感情を表す列と、一個のテキストを表す列が作成できた。\n",
    "# 'text'列を除くすべての列をfloatに変換\n",
    "float_columns = col_names[:-1]  # 最後の'text'列を除くすべての列名\n",
    "df[float_columns] = df[float_columns].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   喜び      0 non-null      float64\n",
      " 1   悲しみ     0 non-null      float64\n",
      " 2   期待      0 non-null      float64\n",
      " 3   驚き      0 non-null      float64\n",
      " 4   怒り      0 non-null      float64\n",
      " 5   恐れ      0 non-null      float64\n",
      " 6   嫌悪      0 non-null      float64\n",
      " 7   信頼      0 non-null      float64\n",
      " 8   text    0 non-null      object \n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 132.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ツイートを感情分析</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "# バッチ処理のためにデータを分割\n",
    "batched_inputs :List[ List[str] ]= [texts_after_treat[i:i + batch_size] for i in range(0, len(texts_after_treat), batch_size)]\n",
    "\"\"\"\n",
    "range(0, len(texts_after_treat), batch_size) : batch_sizeの間隔で 0 から len(texts_after_treat)までデータを取り出す\n",
    "texts_after_treat[i:i + batch_size] : i番目からi + batch_sizeまでのテキストのリスト\n",
    "\"\"\"\n",
    "\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチごとに処理\n",
    "for batch in batched_inputs:\n",
    "    # バッチ内の全テキストを一度にトークナイズ\n",
    "    encoded_input = tokenizer(batch, padding=True, truncation=True, max_length=max_seq_length, return_tensors=\"pt\")\n",
    "    input_ids :torch.Tensor= encoded_input['input_ids']\n",
    "    attention_mask :torch.Tensor= encoded_input['attention_mask']\n",
    "    \n",
    "    # モデルに入力\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # ロジットを取得し、numpy配列に変換\n",
    "    logits :np.ndarray= outputs.logits.detach().numpy()\n",
    "\n",
    "    # バッチ内の各テキストに対する処理\n",
    "    for i, output in enumerate(logits):\n",
    "        tweet :str= batch[i]\n",
    "        output :np.ndarray= output.astype(np.float64)\n",
    "        new_row :List[float | str]= list(output) + [tweet]\n",
    "        df.loc[len(df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('saved_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
